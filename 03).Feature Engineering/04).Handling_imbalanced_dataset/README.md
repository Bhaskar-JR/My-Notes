# Handling Imbalanced Dataset

## What is imbalanced dataset

* Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed; often you'll have a large amount of data/observations for one class (referred to as the majority class), and much fewer observations for one or more other classes (referred to as the minority classes). 

* For example, suppose you're building a classifier to classify a credit card transaction a fraudulent or authentic - you'll likely have 10,000 authentic transactions for every 1 fraudulent transaction, that's quite an imbalance.

## Challenges because of Imbalanced Dataset

* The conventional model evaluation methods do not accurately measure model performance when faced with imbalanced datasets.

* Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards classes which have number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.

* Evaluation of a classification algorithm performance is measured by the Confusion Matrix which contains information about the actual and the predicted class.

* However, while working in an imbalanced domain accuracy is not an appropriate measure to evaluate model performance. For eg: A classifier which achieves an accuracy of 98 % with an event rate of 2 % is not accurate, if it classifies all instances as the majority class. And eliminates the 2 % minority class observations as noise.



## Ways to Handle Imbalanced Dataset

* Random Under-Sampling
* Random Over-Sampling
* SMOTE
* Ensemble Techniques

### 1).Random Under-Sampling
* Under-sampling balances the dataset by reducing the size of the abundant class. 

* This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.


### Advantage
* It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.

### Disadvantages
* It can discard potentially useful information which could be important for building rule classifiers.

* The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the population. Thereby, resulting in inaccurate results with the actual test data set.

### 2).Random Over-Sampling
* On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. 

* Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique)

### Advantages
* Unlike under sampling this method leads to no information loss.

* Outperforms under sampling

### Disadvantages
* It increases the likelihood of overfitting since it replicates the minority class events.


### 3).Synthetic Minority Oversampling Technique (SMOTE)
* This technique generates synthetic data for the minority class.

* SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.

### Advantages 
* Alleviates overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances.

* No loss of information.

* It's simple to implement and interpret.

### Disadvantages 
* While generating synthetic examples, SMOTE does not take into consideration neighboring examples can be from other classes. This can increase the overlapping of classes and can introduce additional noise.

* SMOTE is not very practical for high dimensional data.


### 4).Ensemble Methods 
* Use of ensemble methods is one of the ways to handle the class imbalance problems of the dataset.

* The learning algorithms construct a set of classifiers and then classify new data points by making a choice of their predictions known as Ensemble methods. It has been discovered that ensembles are often much more accurate than the individual classifiers which make them up. 

* Some of the commonly used Ensemble techniques are Bagging or Bootstrap Aggregation, Boosting and Stacking.
